{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Adaptation - a brief review\n",
    "\n",
    "Traditionally, in the supervised learning approach we are interested in approximating a function $f: X \\longrightarrow Y$ by means of using a training sample of the random vectors (or feature space) $X_{\\text{train}}$, this is typically done by optimizing some *cost* function on unseen *test* data.\n",
    "By doing this we are (naively) assuming that both the training and test samples are drawn from the same joint probability distribution. Real world, though, is harsh and this is typically not the case. In particular, once we have trained our ML model, we cannot assure that the *real-world* data that this model will see in production it is drawn from the same joint probability distribuition as the training data. This can happen for several reasons, such as:\n",
    "* Joint probabilities of the domain can, and most probably will, change with time.\n",
    "* Training samples collection might have unknown biases due to different selection methods, for instance.\n",
    "* We might need to generalize a model to a completely new domain, e.g., use training data from Brazil to build a model for Mexico.\n",
    "* ...\n",
    "\n",
    "In all above examples, the training samples are not an accurate representation of the actual examples that the model will see in real-life. Naturally, this will translate into a performance degradation of the model that could worsen with time. To tackle this problem, several techniques under the name of **domain adaptation** were developed. From now on, we will refer to the training and test samples as *source* and *target* distributions. \n",
    "\n",
    "Domain adaptation is a special case of **transfer learning**. Transfer learning refers to a class of machine learning problems where the tasks and/or domains may change between source and target whereas domain adaptation only is interested in problems where the domains differ and tasks remain the same between source and target distributions. Also, semi-supervised learning adresses the problem of having unlabeled data, In this setup, a small set of labeled data is used together to a big amount of unlabeled data to train a model. Moreover, both labeled and unlabeled data are assumed to have being drawn from the same joint probability distribution, an assumption that is relaxed in the domain adaptation and transfer learning setups.\n",
    "\n",
    "In this notebook we will discuss the basic ideias under **Domain adaptation**, the discussion will be motivated by practical (synthetic) examples and at the end we will show how domain adaptation was used on a practical example at the DataLab.\n",
    "Let us begin by stating the problem in general terms by defining the essential mathematical objects under study in this context.\n",
    "\n",
    "# Notation and definitions\n",
    "In domain adaptation, domains consists of three general ingredients:\n",
    "* Input or feature space $X$, e.g. $X \\subset \\mathbb{R}^d$.\n",
    "* output or label sabes $Y$, e.g. $Y = \\{0,1\\}$ for binary classification tasks, $Y = \\{0, 1, \\dots, k-1\\}$ for k-class classification tasks, and $Y \\subset \\mathbb{R}$ for regression tasks.\n",
    "* and an associated joint probability distribution $\\mathcal{D}=\\{X, Y, \\mathbb{P}(X=x,Y=y)\\}$, where $\\mathbb{P}(X=x,Y=y)$ is the probability density.\n",
    "\n",
    "For simplicity we will write $\\mathbb{P}(X= x, Y = y)$ as $\\mathbb{P}(x,y)$. Recall that we can write the joint probability density as:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(x,y) &= \\mathbb{P}(x|y) \\mathbb{P}(y) \\\\\n",
    "& = \\mathbb{P}(y|x)\\mathbb{P}(x)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('domain-adaptation': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2afe8060d8a20fb312f73be7ca782adbe451fdd527bf81a34e84cb65b82e4a89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
